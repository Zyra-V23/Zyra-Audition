# IMPORTANT PREREQUISITES:
# 1. Make sure Ollama is running on your system:
#    - Check with: `ollama serve`
#    - If you see "address already in use", Ollama is already running
#    - If not running, start with: `ollama serve`
# 
# 2. Before using a model, pull it first:
#    - Example: `ollama pull mistral`
#    - Available models: https://ollama.ai/library
#    - Other examples: 
#      * ollama pull llama2
#      * ollama pull codellama
#      * ollama pull phi

# Model Configuration
model:
  # Using models with best performance based on previous tests
  names: ["deepseek-coder:latest"]  # Model optimized for code and analysis
  temperature: 0.1  # Reduced for more deterministic responses
  max_tokens: 1500  # Increased to allow more complete responses
  top_p: 0.9  # Adjusted for better balance

# Prompt Configuration
prompt:
  path: "open_source_examples/prompts/CD_prompt_mejorado.txt"  # Using the improved prompt

# Automation Configuration
automation:
  enabled: true  # Keep automated testing enabled
  parallel: false

# Output Configuration
output:
  format: "csv"
  include_confidence: true
  timestamp_format: "%Y%m%d_%H%M%S"
  output_dir: "data/groups"

# Processing Configuration
processing:
  batch_size: 6  # Reduced for better stability and precision
  max_context_messages: 10  # Increased to provide more context
  min_confidence_threshold: 0.8  # Increased for higher quality

# GPU Configuration
gpu:
  enabled: true
  auto_select: true
  fallback_to_cpu: true 