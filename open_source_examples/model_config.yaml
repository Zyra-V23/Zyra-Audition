# IMPORTANT PREREQUISITES:
# 1. Make sure Ollama is running on your system:
#    - Check with: `ollama serve`
#    - If you see "address already in use", Ollama is already running
#    - If not running, start with: `ollama serve`
# 
# 2. Before using a model, pull it first:
#    - Example: `ollama pull mistral`
#    - Available models: https://ollama.ai/library
#    - Other examples: 
#      * ollama pull llama2
#      * ollama pull codellama
#      * ollama pull phi

# Model Configuration
model:
  names: ["phi:latest", "llama2:latest", "codellama:latest"]  # List of models to use
  temperature: 0.3  # Reduced for more focused outputs
  max_tokens: 1000
  top_p: 0.95  # Slightly increased for better sampling

# Prompt Configuration
prompt:
  path: "open_source_examples/prompts/*"  # Test all available prompts

# Automation Configuration
automation:
  enabled: true  # Enable automated testing
  parallel: false

# Output Configuration
output:
  format: "csv"
  include_confidence: true
  timestamp_format: "%Y%m%d_%H%M%S"
  output_dir: "data/groups"

# Processing Configuration
processing:
  batch_size: 8  # Reduced for better stability
  max_context_messages: 7  # Increased for better context
  min_confidence_threshold: 0.75  # Increased for higher quality predictions

# GPU Configuration
gpu:
  enabled: true
  auto_select: true
  fallback_to_cpu: true 